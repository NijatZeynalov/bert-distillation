{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd9f506-3360-49bd-acaf-199e52e66503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/hf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cc4b8-a6dd-4c4a-9a77-06dcec52be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98050834-880a-433d-827c-cd66cf6ab199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 1: Loading and Preparing Custom Medical Dataset\n",
      "==================================================\n",
      "\n",
      "Train: 2000, Validation: 500, Test: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 1: Loading and Preparing Custom Medical Dataset\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "med_dataset = load_from_disk(\"med_dr_notes\")\n",
    "\n",
    "label_list = ['Dermatology', 'Gastroenterology', 'Endocrinology', 'Oncology', 'Pulmonology']\n",
    "num_labels = len(label_list)\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "\n",
    "def encode_label(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "med_dataset = med_dataset.map(encode_label)\n",
    "\n",
    "train_dataset = med_dataset['train']\n",
    "eval_dataset = med_dataset['test']\n",
    "test_dataset = med_dataset['test']\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(eval_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64bd51-cd58-46dd-bc3d-4679ef6c1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9635e3d7-12cd-4420-bfe7-e06861b52b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: Simplified Tokenization with Compatible Models\n",
      "==================================================\n",
      "\n",
      "Single tokenizer loaded, as models are from the same family.\n",
      "\n",
      "Data tokenized successfully using a single tokenizer.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: Simplified Tokenization with Compatible Models\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "teacher_id = \"bert-base-uncased\"\n",
    "student_id = \"boltuix/bert-micro\"  \n",
    "\n",
    "# Since both models share the same tokenizer, we only need to load it once.\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n",
    "print(\"Single tokenizer loaded, as models are from the same family.\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "print(\"\\nData tokenized successfully using a single tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed442ba-4ecc-44e8-8b62-cb4729ceabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 3: Fine-tuning Teacher Model (bert-base-uncased)\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.740900</td>\n",
       "      <td>0.443258</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.850910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.311850</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.891510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.905635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher Model Evaluation: {'eval_loss': 0.31288275122642517, 'eval_accuracy': 0.906, 'eval_f1': 0.905634712405052, 'eval_runtime': 6.9595, 'eval_samples_per_second': 71.844, 'eval_steps_per_second': 4.598, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"STEP 3: Fine-tuning Teacher Model ({teacher_id})\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    acc = evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "teacher_training_args = TrainingArguments(\n",
    "    output_dir=\"models/teacher_bert_med_notes\",\n",
    "    num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    logging_steps=50, eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"f1\", report_to=\"none\"\n",
    ")\n",
    "teacher_trainer = Trainer(\n",
    "    model=teacher_model, args=teacher_training_args, train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval, compute_metrics=compute_metrics\n",
    ")\n",
    "teacher_trainer.train()\n",
    "teacher_eval_results = teacher_trainer.evaluate(tokenized_test)\n",
    "\n",
    "print(f\"\\nTeacher Model Evaluation: {teacher_eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f392420f-b9aa-4973-988f-d12cbad42af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: Distilling Knowledge into Student (boltuix/bert-micro)\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at boltuix/bert-micro and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.381900</td>\n",
       "      <td>2.976609</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.728221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.610100</td>\n",
       "      <td>2.334731</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.756328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.209700</td>\n",
       "      <td>1.953837</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.771680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.943800</td>\n",
       "      <td>1.744463</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.791012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.820400</td>\n",
       "      <td>1.676093</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.806528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distilled Student Model Evaluation: {'eval_loss': 1.6760931015014648, 'eval_accuracy': 0.82, 'eval_f1': 0.8065280551652521, 'eval_runtime': 7.3942, 'eval_samples_per_second': 67.62, 'eval_steps_per_second': 4.328, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"STEP 4: Distilling Knowledge into Student ({student_id})\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        if self.teacher_model:\n",
    "            self.teacher_model.to(self.args.device)\n",
    "            self.teacher_model.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch = None):\n",
    "        student_outputs = model(**inputs)\n",
    "        student_loss = student_outputs.loss\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "        distillation_loss = F.kl_div(\n",
    "            F.log_softmax(student_outputs.logits / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_outputs.logits / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "        loss = self.alpha * student_loss + (1.0 - self.alpha) * distillation_loss\n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "\n",
    "# Load student model\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_id, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "student_training_args = TrainingArguments(\n",
    "    output_dir=\"models/student_bert_micro_med_notes\", # <--- CHANGE HERE: New output directory\n",
    "    num_train_epochs=5, # A much smaller model might benefit from more training epochs\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "distillation_trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=student_training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    alpha=0.2, # Giving even more weight to the teacher (70%) as the student is very small\n",
    "    temperature=3.0\n",
    ")\n",
    "\n",
    "distillation_trainer.train()\n",
    "student_eval_results = distillation_trainer.evaluate(tokenized_test)\n",
    "print(f\"\\nDistilled Student Model Evaluation: {student_eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0ba93e-ec74-4a62-8b80-22579c93521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 5: Final Results and Comparison\n",
      "==================================================\n",
      "\n",
      "Teacher Model ('bert-base-uncased') size: 109.49M parameters\n",
      "Student Model ('boltuix/bert-micro') size: 4.39M parameters\n",
      "Size Reduction: 95.99%\n",
      "\n",
      "--- Performance on Medical Notes Test Set ---\n",
      "Model                               | Macro F1-Score \n",
      "-------------------------------------------------------\n",
      "1. Fine-tuned Teacher (BERT-base)   | 0.9056         \n",
      "2. Distilled Student (BERT-micro)   | 0.8065         \n",
      "-------------------------------------------------------\n",
      "\n",
      "Performance Retained: The distilled student retained 89.06% of the teacher's F1-score.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Final Results and Comparison\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "teacher_params = teacher_model.num_parameters() / 1_000_000\n",
    "student_params = student_model.num_parameters() / 1_000_000\n",
    "\n",
    "print(f\"Teacher Model ('{teacher_id}') size: {teacher_params:.2f}M parameters\")\n",
    "print(f\"Student Model ('{student_id}') size: {student_params:.2f}M parameters\")\n",
    "print(f\"Size Reduction: {100 * (1 - student_params / teacher_params):.2f}%\\n\")\n",
    "\n",
    "teacher_f1 = teacher_eval_results['eval_f1']\n",
    "student_f1 = student_eval_results['eval_f1']\n",
    "performance_retention = (student_f1 / teacher_f1) * 100\n",
    "\n",
    "print(\"--- Performance on Medical Notes Test Set ---\")\n",
    "print(f\"{'Model':<35} | {'Macro F1-Score':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'1. Fine-tuned Teacher (BERT-base)':<35} | {teacher_f1:<15.4f}\")\n",
    "print(f\"{'2. Distilled Student (BERT-micro)':<35} | {student_f1:<15.4f}\") \n",
    "print(\"-\" * 55)\n",
    "print(f\"\\nPerformance Retained: The distilled student retained {performance_retention:.2f}% of the teacher's F1-score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b933201f-ae26-4dc1-a11f-8b1e37bcbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_MODEL_PATH = \"models/teacher_bert_med_notes/checkpoint-375\"\n",
    "STUDENT_MODEL_PATH = \"models/student_bert_micro_med_notes/checkpoint-625\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5456f5a4-4df9-4350-99d2-1289f7fd5dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best teacher model from: models/teacher_bert_med_notes/checkpoint-375\n",
      "Loading best student model from: models/student_bert_micro_med_notes/checkpoint-625\n",
      "\n",
      "--- Evaluating: Teacher (bert-base-uncased) on 500 examples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing with Teacher (bert-base-uncased): 100%|█████████████████████████████████████| 16/16 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating: Student (prajjwal1/bert-micro) on 500 examples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing with Student (prajjwal1/bert-micro): 100%|█████████████████████████████████| 16/16 [00:00<00:00, 269.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================================\n",
      "Final Comparison: Performance, Size, and Speed on Medical Notes Test Set\n",
      "=====================================================================================\n",
      "Model                          | Macro F1   | Params (M)   | Time (s)   | Latency (ms/ex)  | RTF     \n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Teacher (bert-base-uncased)    | 0.9056      | 109.49       | 0.72       | 1.45           | 0.0000\n",
      "Student (prajjwal1/bert-micro) | 0.8065      | 4.39         | 0.02       | 0.03           | 0.0000\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Summary ---\n",
      "Size Reduction: The student model is 95.99% smaller than the teacher.\n",
      "Performance Retained: The student retained 89.06% of the teacher's F1-score.\n",
      "Inference Speedup: The student is 46.38x faster than the teacher.\n",
      "RTF Interpretation: An RTF < 1.0 means the model processes text faster than real-time speech.\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Load the best models saved during training ---\n",
    "print(f\"Loading best teacher model from: {TEACHER_MODEL_PATH}\")\n",
    "teacher_model_loaded = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_PATH).to(device)\n",
    "\n",
    "print(f\"Loading best student model from: {STUDENT_MODEL_PATH}\")\n",
    "student_model_loaded = AutoModelForSequenceClassification.from_pretrained(STUDENT_MODEL_PATH).to(device)\n",
    "\n",
    "\n",
    "# --- Helper Function for Comprehensive Evaluation ---\n",
    "def evaluate_model_performance(model_name, model, dataset, raw_text_list):\n",
    "    \"\"\"\n",
    "    Evaluates a model for performance and speed, including RTF, on the full test set.\n",
    "    Returns a dictionary with all calculated metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating: {model_name} on {len(dataset)} examples ---\")\n",
    "    model.eval()\n",
    "    # We need to re-add the 'label' column for the dataloader\n",
    "    dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    data_loader = DataLoader(dataset, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=f\"Inferencing with {model_name}\"):\n",
    "        labels = batch.pop('label').to(device) # .pop() is crucial here\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_time = time.perf_counter()\n",
    "            outputs = model(**inputs)\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "        all_logits.append(outputs.logits.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    logits_array = np.concatenate(all_logits)\n",
    "    labels_array = np.concatenate(all_labels)\n",
    "    perf_metrics = compute_metrics((logits_array, labels_array))\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    avg_latency_ms = (total_inference_time / num_samples) * 1000\n",
    "    throughput_sps = num_samples / total_inference_time\n",
    "\n",
    "    # Calculate Real-Time Factor (RTF) assuming 2.5 words/sec human speech rate\n",
    "    words_per_second_human = 2.5\n",
    "    total_words = sum(len(text.split()) for text in raw_text_list)\n",
    "    simulated_duration_sec = total_words / words_per_second_human\n",
    "    rtf = total_inference_time / simulated_duration_sec\n",
    "\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"f1\": perf_metrics[\"f1\"],\n",
    "        \"accuracy\": perf_metrics[\"accuracy\"],\n",
    "        \"params_m\": model.num_parameters() / 1_000_000,\n",
    "        \"total_time_s\": total_inference_time,\n",
    "        \"latency_ms\": avg_latency_ms,\n",
    "        \"throughput_sps\": throughput_sps,\n",
    "        \"rtf\": rtf\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Use the original test_dataset to get the raw text for RTF calculation\n",
    "raw_test_text = test_dataset['text']\n",
    "\n",
    "# --- Run Evaluations on the full test set ---\n",
    "teacher_results = evaluate_model_performance(\n",
    "    f\"Teacher ({teacher_id})\", teacher_model_loaded, tokenized_test, raw_test_text\n",
    ")\n",
    "student_results = evaluate_model_performance(\n",
    "    f\"Student ({student_id})\", student_model_loaded, tokenized_test, raw_test_text\n",
    ")\n",
    "\n",
    "\n",
    "# --- Display Final Comparison Table ---\n",
    "print(\"\\n\" + \"=\"*85)\n",
    "print(\"Final Comparison: Performance, Size, and Speed on Medical Notes Test Set\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "header = f\"{'Model':<30} | {'Macro F1':<10} | {'Params (M)':<12} | {'Time (s)':<10} | {'Latency (ms/ex)':<16} | {'RTF':<8}\"\n",
    "separator = \"-\" * len(header)\n",
    "\n",
    "print(header)\n",
    "print(separator)\n",
    "\n",
    "print(f\"{teacher_results['model_name']:<30} | {teacher_results['f1']:.4f}{'':<5} | \"\n",
    "      f\"{teacher_results['params_m']:<12.2f} | {teacher_results['total_time_s']:<10.2f} | \"\n",
    "      f\"{teacher_results['latency_ms']:.2f}{'':<10} | {teacher_results['rtf']:.4f}\")\n",
    "\n",
    "print(f\"{student_results['model_name']:<30} | {student_results['f1']:.4f}{'':<5} | \"\n",
    "      f\"{student_results['params_m']:<12.2f} | {student_results['total_time_s']:<10.2f} | \"\n",
    "      f\"{student_results['latency_ms']:.2f}{'':<10} | {student_results['rtf']:.4f}\")\n",
    "\n",
    "print(separator)\n",
    "\n",
    "# --- Summary & Interpretation ---\n",
    "size_reduction = (1 - student_results['params_m'] / teacher_results['params_m']) * 100\n",
    "performance_retention = (student_results['f1'] / teacher_results['f1']) * 100\n",
    "speedup_factor = teacher_results['total_time_s'] / student_results['total_time_s']\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Size Reduction: The student model is {size_reduction:.2f}% smaller than the teacher.\")\n",
    "print(f\"Performance Retained: The student retained {performance_retention:.2f}% of the teacher's F1-score.\")\n",
    "print(f\"Inference Speedup: The student is {speedup_factor:.2f}x faster than the teacher.\")\n",
    "print(f\"RTF Interpretation: An RTF < 1.0 means the model processes text faster than real-time speech.\")\n",
    "print(\"=\"*85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
